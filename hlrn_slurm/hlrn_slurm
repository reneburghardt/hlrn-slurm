#!/usr/bin/env python3

import yaml
import os
import stat
import subprocess
from pathlib import Path
from time import sleep
from jsonargparse import ArgumentParser, ActionConfigFile
import yaml
from pathlib import Path
import copy


def find_sweep_params(sweep_config):
    sweep_params = {}
    for k, v in sweep_config.items():
        if k.startswith("_sweep_"):
            sweep_params[k] = v
        elif isinstance(v, dict):
            sub_params = find_sweep_params(v)
            if sub_params:
                sweep_params[k] = sub_params
    return sweep_params


def flatten_dict(d):
    out = {}
    for k, v in d.items():
        if isinstance(v, dict):
            sub_dict = flatten_dict(v)
            for sub_k, sub_v in sub_dict.items():
                out[f"{k}.{sub_k}"] = sub_v
        else:
            out[k] = v
    return out


def generate_sweep(sweep_config, sweep_path):
    sweep_path = Path(sweep_path)
    # strip file ending from sweep_path and add "_sweep_" to the end
    sweep_path = sweep_path.with_suffix("")
    # sweep_path = sweep_path.with_name(f"{sweep_path.name}_sweep")
    if sweep_path.exists():
        print(
            f"Sweep directory {sweep_path} already exists. Running only existing sweep configs."
        )
        return list(sweep_path.glob("*.yaml"))
    # create sweep directory
    sweep_path.mkdir(parents=True, exist_ok=False)

    # find all parameters that start with "_sweep_" and generate a list of all possible combinations
    # recursively find all parameters that start with "_sweep_"
    sweep_params = find_sweep_params(sweep_config)
    # flatten the dictionary
    sweep_params = flatten_dict(sweep_params)

    # generate all possible combinations of parameters
    import itertools

    keys, values = zip(*sweep_params.items())
    sweep_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]

    # create new config for each combination
    config_files = []
    for i, sweep_combination in enumerate(sweep_combinations):
        new_config = copy.deepcopy(sweep_config)
        for k, v in sweep_combination.items():
            # find the key in the config
            sub_dict = new_config
            keys = k.split(".")
            for key in keys[:-1]:
                sub_dict = sub_dict[key]

            # strip "_sweep_" from the key and set the value
            key = keys[-1]
            del sub_dict[key]
            key = key.replace("_sweep_", "")
            sub_dict[key] = v
        # write config to file
        yaml_file = Path(f"{sweep_path}/sweep_{i}.yaml")
        yaml_file.write_text(yaml.dump(new_config))
        config_files.append(yaml_file)
    return config_files


class SlurmJob:
    def __init__(
        self,
        config,
        index=0,
        config_file="",
    ):
        self.name = f"{config.name}-{index}"
        self.account = config.account if config.account else os.environ.get("USER", "")
        self.gpu = f"{config.gpu}:{config.ngpus}"
        self.bindings = config.bindings
        if config_file:
            self.bindings["/local/$USER/$SLURM_JOB_ID/"] = "/config_folder"
        self.config_file = config_file
        self.config = config

    @property
    def resource_config_string(self):
        if not Path("logs").exists():
            os.mkdir("logs")
        config_string = f"""
#SBATCH --account={self.account}
#SBATCH --job-name={self.name}                   # Name of the job
#SBATCH --ntasks=1                          # Number of tasks
#SBATCH --cpus-per-task={self.config.ncpus}                   # Number of CPU cores per task
#SBATCH --nodes=1                           # Ensure that all cores are on one machine
#SBATCH --time={self.config.time}                       # Runtime in D-HH:MM
#SBATCH --mem-per-cpu={self.config.memory}              # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --output=logs/{self.name}.%j.out              # File to which STDOUT will be written
#SBATCH --error=logs/{self.name}.%j.err               # File to which STDERR will be written
#SBATCH --mail-type=ALL                     # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user={self.config.email}                 # Email to which notifications will be sent
#SBATCH -p {self.config.partition}                   # Partition to submit to
#SBATCH -G {self.gpu}                         # Number of requested GPUs
#SBATCH --exclude={self.config.exclude}         # Exclude nodes
            """
        if int(self.config.time.split("-")[0]) > 2:
            config_string += """
#SBATCH --qos=96h     # allow for longer runs
                """
        return config_string

    @property
    def singularity_run_command(self):
        run_cmd = ""
        for outside_path in self.bindings.keys():
            if not Path(outside_path).exists():
                run_cmd += f"mkdir -p {outside_path}\n"
        if not self.config.local:
            run_cmd += f"""
                module load singularity
                module load cuda/{self.config.cuda}
                scontrol show job $SLURM_JOB_ID  # print some info
            """

        if self.config_file:
            run_cmd += f"""
                cp {self.config_file} /local/$USER/$SLURM_JOB_ID/config.yaml
            """
            run_args = (
                self.config.run_args + " --experiment-file /config_folder/config.yaml"
            )
        else:
            run_args = self.config.run_args
        singularity_mode = "instance start" if self.config.interactive else "exec"
        instance_name = self.name if self.config.interactive else ""
        bindings = ",".join(
            [f"{src}:{dest}" for src, dest in self.config.bindings.items()]
        )
        run_cmd += f""" 
            singularity {singularity_mode} \
            --nv \
            --env-file {self.config.env} \
            --no-home  \
            --bind {bindings}  \
            {self.config.img} {instance_name} \
            {self.config.run} {run_args}
            """
        if self.config.interactive:
            run_cmd += "sleep infinity"
        return run_cmd

    def run(self):
        if not self.config.local:
            slurm_job_bash_file = f"./{self.name}.sh"
            slurm_job_bash_file_content = (
                "#!/bin/bash \n \n"
                + self.resource_config_string
                + "\n"
                + self.singularity_run_command
            )
            with open(slurm_job_bash_file, "w") as f:
                f.write(slurm_job_bash_file_content)

            os.chmod(slurm_job_bash_file, stat.S_IRWXU)

            try:
                output = subprocess.check_output(
                    "sbatch " + slurm_job_bash_file, shell=True
                )
                sleep(5)  # wait for the job to be submitted
                job_id = int(output[20:].strip())
                node = subprocess.check_output(
                    f"scontrol show job {job_id}| grep ' NodeList'", shell=True
                ).strip()
                node = str(node).split("=")[1][:-1]
                print(f"Successfully submitted job with ID {job_id} to node {node}.")
                print(self.resource_config_string)
                print(self.singularity_run_command)
            finally:
                # remove the bash file
                os.remove(slurm_job_bash_file)
        else:
            print(self.singularity_run_command)
            print(subprocess.check_output(self.singularity_run_command, shell=True))


if __name__ == "__main__":
    parser = ArgumentParser(description="Running jobs on SLURM cluster")
    parser.add_argument("--cfg", action=ActionConfigFile)
    parser.add_argument(
        "--run",
        dest="run",
        default="run.py",
        type=str,
        help="",
    )
    parser.add_argument(
        "--njobs",
        dest="njobs",
        default=1,
        type=int,
        help="",
    )
    parser.add_argument(
        "--account",
        dest="account",
        default="",
        type=str,
    )
    parser.add_argument(
        "--name",
        dest="name",
        default="noname",
        type=str,
        help="",
    )
    parser.add_argument(
        "--time",
        dest="time",
        default="0-01:00",
        type=str,
        help="time to complete each job. Specify in the following format: D-HH:MM",
    )
    parser.add_argument(
        "--gpu",
        dest="gpu",
        default="A100",
        type=str,
        help="",
    )
    parser.add_argument(
        "--partition",
        dest="partition",
        default="grete:shared",
        type=str,
        help="",
    )
    parser.add_argument(
        "--ncpus",
        dest="ncpus",
        default=2,
        type=int,
        help="",
    )
    parser.add_argument(
        "--ngpus",
        dest="ngpus",
        default=1,
        type=int,
        help="",
    )
    parser.add_argument(
        "--memory",
        dest="memory",
        default="3G",
        type=str,
        help="",
    )
    parser.add_argument(
        "--email",
        dest="email",
        default=os.getenv("EMAIL"),
        type=str,
        help="",
    )
    parser.add_argument(
        "--local",
        dest="local",
        default=False,
        action="store_true",
        help="Specify whether this is a job on SLURM or a local machine.",
    )
    parser.add_argument(
        "--img",
        dest="img",
        default="shub://sinzlab/pytorch-singularity:v3.8-torch1.7.0-dj0.12.7",
        type=str,
        help="Singularity image to use",
    )
    parser.add_argument(
        "--cuda",
        dest="cuda",
        default="11.2",
        type=str,
        help="Cuda version to use",
    )
    parser.add_argument(
        "--env",
        dest="env",
        default=".env",
        type=str,
        help="Environment file",
    )
    parser.add_argument(
        "--bindings",
        dest="bindings",
        default={},
        type=dict,
        help="List of bindings",
    )
    parser.add_argument(
        "--exclude",
        dest="exclude",
        default="",
        type=str,
        help="List of nodes to exclude",
    )
    parser.add_argument(
        "--sweep",
        dest="sweep",
        default="",
        type=str,
        help="Sweep file",
    )
    parser.add_argument(
        "--interactive",
        dest="interactive",
        action="store_true",
        default=False,
        help="Start a container that does not run a command, but is interactive (i.e. you can connect into it for development).",
    )
    parser.add_argument(
        "--run_args",
        dest="run_args",
        default="",
        type=str,
        help="Arguments to pass to the run command",
    )
    config = parser.parse_args()

    if config.sweep:
        yaml_file = Path(f"{config.sweep}")
        if not yaml_file.exists():
            raise ValueError(f"Sweep file {yaml_file} does not exist")
        sweep_config = yaml.safe_load(yaml_file.read_text())
        config_files = generate_sweep(sweep_config, config.sweep)
        for i, config_file in enumerate(config_files):
            job = SlurmJob(
                config,
                index=i,
                config_file=config_file,
            )
            job.run()
    else:
        for job_index in range(config.njobs):
            job = SlurmJob(
                config,
                index=job_index,
            )
            job.run()
