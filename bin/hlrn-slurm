#!/usr/bin/env python3

from utils.sweep import generate_sweep

import yaml
import os
import stat
import subprocess
from pathlib import Path
from time import sleep
from jsonargparse import ArgumentParser, ActionConfigFile


class SlurmJob:
    def __init__(
        self,
        config,
        index=0,
        config_file="",
    ):
        self.name = f"{config.name}-{index}"
        self.account = config.account if config.account else os.environ.get("USER", "")
        self.gpu = f"{config.gpu}:{config.ngpus}"
        self.bindings = config.bindings
        if config_file:
            self.bindings["/local/$USER/$SLURM_JOB_ID/"] = "/config_folder"
        self.config_file = config_file
        self.config = config

    @property
    def resource_config_string(self):
        if not Path("logs").exists():
            os.mkdir("logs")
        config_string = f"""
#SBATCH --account={self.account}
#SBATCH --job-name={self.name}                   # Name of the job
#SBATCH --ntasks=1                          # Number of tasks
#SBATCH --cpus-per-task={self.config.ncpus}                   # Number of CPU cores per task
#SBATCH --nodes=1                           # Ensure that all cores are on one machine
#SBATCH --time={self.config.time}                       # Runtime in D-HH:MM
#SBATCH --mem-per-cpu={self.config.memory}              # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --output=logs/{self.name}.%j.out              # File to which STDOUT will be written
#SBATCH --error=logs/{self.name}.%j.err               # File to which STDERR will be written
#SBATCH --mail-type=ALL                     # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user={self.config.email}                 # Email to which notifications will be sent
#SBATCH -p {self.config.partition}                   # Partition to submit to
#SBATCH -G {self.gpu}                         # Number of requested GPUs
#SBATCH --exclude={self.config.exclude}         # Exclude nodes
            """
        if int(self.config.time.split("-")[0]) > 2:
            config_string += """
#SBATCH --qos=96h     # allow for longer runs
                """
        return config_string

    @property
    def singularity_run_command(self):
        run_cmd = ""
        for outside_path in self.bindings.keys():
            if not Path(outside_path).exists():
                run_cmd += f"mkdir -p {outside_path}\n"
        if not self.config.local:
            run_cmd += f"""
                module load singularity
                module load cuda/{self.config.cuda}
                scontrol show job $SLURM_JOB_ID  # print some info
            """

        if self.config_file:
            run_cmd += f"""
                cp {self.config_file} /local/$USER/$SLURM_JOB_ID/config.yaml
            """
            run_args = (
                self.config.run_args + " --experiment-file /config_folder/config.yaml"
            )
        else:
            run_args = self.config.run_args
        singularity_mode = "instance start" if self.config.interactive else "exec"
        instance_name = self.name if self.config.interactive else ""
        bindings = ",".join(
            [f"{src}:{dest}" for src, dest in self.config.bindings.items()]
        )
        run_cmd += f""" 
            singularity {singularity_mode} \
            --nv \
            --env-file {self.config.env} \
            --no-home  \
            --bind {bindings}  \
            {self.config.img} {instance_name} \
            {self.config.run} {run_args}
            """
        if self.config.interactive:
            run_cmd += "sleep infinity"
        return run_cmd

    def run(self):
        if not self.config.local:
            slurm_job_bash_file = f"./{self.name}.sh"
            slurm_job_bash_file_content = (
                "#!/bin/bash \n \n"
                + self.resource_config_string
                + "\n"
                + self.singularity_run_command
            )
            with open(slurm_job_bash_file, "w") as f:
                f.write(slurm_job_bash_file_content)

            os.chmod(slurm_job_bash_file, stat.S_IRWXU)

            try:
                output = subprocess.check_output(
                    "sbatch " + slurm_job_bash_file, shell=True
                )
                sleep(5)  # wait for the job to be submitted
                job_id = int(output[20:].strip())
                node = subprocess.check_output(
                    f"scontrol show job {job_id}| grep ' NodeList'", shell=True
                ).strip()
                node = str(node).split("=")[1][:-1]
                print(f"Successfully submitted job with ID {job_id} to node {node}.")
                print(self.resource_config_string)
                print(self.singularity_run_command)
            finally:
                # remove the bash file
                os.remove(slurm_job_bash_file)
        else:
            print(self.singularity_run_command)
            print(subprocess.check_output(self.singularity_run_command, shell=True))


if __name__ == "__main__":
    parser = ArgumentParser(description="Running jobs on SLURM cluster")
    parser.add_argument("--cfg", action=ActionConfigFile)
    parser.add_argument(
        "--run",
        dest="run",
        default="run.py",
        type=str,
        help="",
    )
    parser.add_argument(
        "--njobs",
        dest="njobs",
        default=1,
        type=int,
        help="",
    )
    parser.add_argument(
        "--account",
        dest="account",
        default="",
        type=str,
    )
    parser.add_argument(
        "--name",
        dest="name",
        default="noname",
        type=str,
        help="",
    )
    parser.add_argument(
        "--time",
        dest="time",
        default="0-01:00",
        type=str,
        help="time to complete each job. Specify in the following format: D-HH:MM",
    )
    parser.add_argument(
        "--gpu",
        dest="gpu",
        default="A100",
        type=str,
        help="",
    )
    parser.add_argument(
        "--partition",
        dest="partition",
        default="grete:shared",
        type=str,
        help="",
    )
    parser.add_argument(
        "--ncpus",
        dest="ncpus",
        default=2,
        type=int,
        help="",
    )
    parser.add_argument(
        "--ngpus",
        dest="ngpus",
        default=1,
        type=int,
        help="",
    )
    parser.add_argument(
        "--memory",
        dest="memory",
        default="3G",
        type=str,
        help="",
    )
    parser.add_argument(
        "--email",
        dest="email",
        default=os.getenv("EMAIL"),
        type=str,
        help="",
    )
    parser.add_argument(
        "--local",
        dest="local",
        default=False,
        action="store_true",
        help="Specify whether this is a job on SLURM or a local machine.",
    )
    parser.add_argument(
        "--img",
        dest="img",
        default="shub://sinzlab/pytorch-singularity:v3.8-torch1.7.0-dj0.12.7",
        type=str,
        help="Singularity image to use",
    )
    parser.add_argument(
        "--cuda",
        dest="cuda",
        default="11.2",
        type=str,
        help="Cuda version to use",
    )
    parser.add_argument(
        "--env",
        dest="env",
        default=".env",
        type=str,
        help="Environment file",
    )
    parser.add_argument(
        "--bindings",
        dest="bindings",
        default={},
        type=dict,
        help="List of bindings",
    )
    parser.add_argument(
        "--exclude",
        dest="exclude",
        default="",
        type=str,
        help="List of nodes to exclude",
    )
    parser.add_argument(
        "--sweep",
        dest="sweep",
        default="",
        type=str,
        help="Sweep file",
    )
    parser.add_argument(
        "--interactive",
        dest="interactive",
        action="store_true",
        default=False,
        help="Start a container that does not run a command, but is interactive (i.e. you can connect into it for development).",
    )
    parser.add_argument(
        "--run_args",
        dest="run_args",
        default="",
        type=str,
        help="Arguments to pass to the run command",
    )
    config = parser.parse_args()

    if config.sweep:
        yaml_file = Path(f"{config.sweep}")
        if not yaml_file.exists():
            raise ValueError(f"Sweep file {yaml_file} does not exist")
        sweep_config = yaml.safe_load(yaml_file.read_text())
        config_files = generate_sweep(sweep_config, config.sweep)
        for i, config_file in enumerate(config_files):
            job = SlurmJob(
                config,
                index=i,
                config_file=config_file,
            )
            job.run()
    else:
        for job_index in range(config.njobs):
            job = SlurmJob(
                config,
                index=job_index,
            )
            job.run()
